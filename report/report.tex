\documentclass[abstract=on,9pt,twocolumn]{scrartcl}

\usepackage{ucs}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{datetime}
%\usepackage{multicol}
\usepackage{float}

\usepackage[paper=a4paper,top=2cm,left=1.5cm,right=1.5cm,bottom=2cm,foot=1cm]{geometry}

\usepackage{relsize}%	relative font sizes

\usepackage[retainorgcmds]{IEEEtrantools}%	IEEEeqnarray
\setlength{\IEEEnormaljot}{4\IEEEnormaljot}

\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{indentfirst}
\usepackage[hyphens]{url}
\usepackage[linktocpage]{hyperref}
%\usepackage{hyperref}
%\usepackage{cleveref}
\usepackage[noabbrev]{cleveref}
\usepackage{listings}
\usepackage{color}
\usepackage{subfig}

%%%%%%%%%%%%%%%%
%  title page  %
%%%%%%%%%%%%%%%%
\titlehead{University of Texas at Austin \hfill Institute for Computational Engineering and Sciences (ICES)}
\title{GMetis - Xeon Phi}

\author{
    \\David Pereira\\
     	\texttt{\smaller pg22821@alunos.uminho.pt}
\\~\\~
\and\\Rui Brito\\
	\texttt{\smaller pg22781@alunos.uminho.pt}
}

\date{Austin, \docdate}

%%%%%%%%%%%
%  Hacks  %
%%%%%%%%%%%

%	Paragraph (title) with linebreak
\newcommand{\paragraphh}[1]{\paragraph{#1\hfill}\hfill

}

%	Add "Appendix" to the appendices titles, but not to the references
\usepackage{ifthen}
\newcommand*{\appendixmore}{%
  \renewcommand*{\othersectionlevelsformat}[1]{%
    \ifthenelse{\equal{##1}{section}}{\appendixname~}{}%
    \csname the##1\endcsname\autodot\enskip}
  \renewcommand*{\sectionmarkformat}{%
    \appendixname~\thesection\autodot\enskip}
}

\newdateformat{mmmyyyydate}{\monthname[\THEMONTH] \THEYEAR}
\newcommand{\docdate}{\mmmyyyydate\today}


%-----------------------------------------------------------------------------
% Cenas a falar
%-----------------------------------------------------------------------------

% USA W -> o maior grafo possivel de correr no MIC devido a limitações
% de memória

% Correr o Metis num core do MIC? Em vez de correr no Host?



%-----------------------------------------------------------------------------
% Begin Document
%-----------------------------------------------------------------------------

\begin{document}
\maketitle	


%-----------------------------------------------------------------------------
% Abstract
%-----------------------------------------------------------------------------

\begin{abstract}

\end{abstract}


%-----------------------------------------------------------------------------
% Introduction
%-----------------------------------------------------------------------------

\section{Introduction}


\begin{itemize}
  \item GMetis is a graph partitioning application which uses the Galois
    framework
  \item Consists of three major phases
    \begin{itemize}
      \item Coarsening
      \begin{itemize}
        \item Find matching nodes
        \item Create Coarse Edges
      \end{itemize}
      \item Initial Partitioning (Clustering)
      \item Refinement
    \end{itemize}
\end{itemize}



%-----------------------------------------------------------------------------
% Metis Algorithm Description
%-----------------------------------------------------------------------------

\section{The Metis Algorithm}
\label{sec:metis_alg}

  Formally, the metis algorithm consists of three phases. They are as follows:

  \begin{itemize}
  \item Given a graph $G_0 = (V_0,E_0)$:
  \begin{itemize}    
    \item Coarsening:
    \begin{itemize}
      \item $G_0$ is transformed into a sequence of smaller graphs $G_1,G_2,\cdots,G_m$ such that $|V_0|>|V_1|>|V_2|>\cdots>|V_m|$
    \end{itemize}
    \item Partitioning: 
    \begin{itemize}
      \item A 2-way partition $P_m$ of the graph $G_m = (V_m,E_m)$ is computed that partitions $V_m$ into two parts, each containing half the vertices of $G_0$
    \end{itemize}
    \item Refinement:
    \begin{itemize}
      \item The partition $P_m$ of $G_m$ is projected back to $G_0$ by going through intermediate partitions $P_{m-1}, P_{m-2},\cdots,P_1,P_0$
    \end{itemize}
  \end{itemize}
\end{itemize}

Visually, this translates into the following scenarios:

\begin{center}
  \begin{figure}[H]
    \includegraphics[width=\columnwidth]{img/coarsening.eps}
    \caption{Initial graph}
    \label{img:init_graph}
  \end{figure} 
\end{center}

Figures \ref{img:init_graph} and \ref{img:coarse_graph} illustrate the coarsening phase. During this phase, a sequence of coarser graphs is constructed.\cite{Karypis95parallelmultilevel} A coarser graph is constructed by matching neighbour vertices and then contracting the edges.

\begin{center}
  \begin{figure}[htb]
    \includegraphics[width=\columnwidth]{img/coarsening2.eps}
    \caption{Coarsened graph}
    \label{img:coarse_graph}
  \end{figure}
\end{center}

Figure \ref{img:part_graph} displays the partitioned graph, this is the next step in the algorithm. To do this, a Greedy Graph Growing (GGGP) algorithm is used.\cite{Karypis:1998:FHQ:305219.305248} The goal of this phase, is to compute a high quality bisection (e.g., small edge-cut) of the coarsened graph such that each part contains roughly half of the vertices and edges of the original graph.
\begin{center}
  \begin{figure}[htb]
    \includegraphics[width=\columnwidth]{img/partition.eps}
    \caption{Partitioned graph}
    \label{img:part_graph}
  \end{figure}
\end{center}

Figure \ref{img:refined_graph} shows the results of the refinement phase. During this stage, the partition of the coarser graph is projected back to the original graph by going through the graphs.\cite{Karypis:1998:FHQ:305219.305248}
Once again, the goal here, is to minimize the edge-cut, however, a good balance in the number of vertices assigned to each partition is also very important. Hence, in this final phase, some algorithms use special heuristics to further improve on the balancing achieved.
\begin{center}
  \begin{figure}[htb]
    \includegraphics[width=\columnwidth]{img/refinement.eps}
    \caption{Refined graph}
    \label{img:refined_graph} 
  \end{figure}
\end{center}


%-----------------------------------------------------------------------------
% System characteristics
%-----------------------------------------------------------------------------

\section{System characteristics}
\label{sec:sys_char}

The measurements were performed in both Stampede's hosts and
coprocessors.
The hosts are comprised of dual Intel Xeon E5-2680, while the
coprocessors are the new Intel Xeon Phi with 61 cores. Their
characteristics are presented in the following tables.

\begin{table}[H]
\centering
\footnotesize
\begin{tabular}{| c | c |}\hline
Manufacturer & Intel\\ \hline
Model & Xeon E5-2680\\ \hline
$\mu$Arch & Sandy Bridge\\ \hline
Clock freq & 2.70 GHz\\ \hline
\#CPUs (sockets) & 2 \\ \hline
\#Cores/CPU & 8\\ \hline
\#Thread/Core & 1\\ \hline
L1 cache size/core & 32 KB\\ \hline
L2 cache size/core & 256 KB\\ \hline
L3 shared cache size/CPU & 20 MB\\ \hline
Main Memory/CPU & 16 GB\\ \hline
Vector width & 256 bits (AVX)\\ \hline
\end{tabular}
\label{tab:host_stampede}
\caption{Intel Xeon E5-2680}
\end{table}

\begin{table}[H]
\centering
\footnotesize
\begin{tabular}{| c | c |}\hline
Manufacturer & Intel\\ \hline
Model & Xeon Phi SE10P\\ \hline
$\mu$Arch & Many Integrated Cores - MIC\\ \hline
Clock freq & 1.1 GHz\\ \hline
\#CPUs (sockets) & 1 \\ \hline
\#Cores/CPU & 61\\ \hline
\#Thread/Core & 4\\ \hline
L1 cache size/core & 32KB\\ \hline
L2 cache size/core & 512 KB\\ \hline
Main Memory/CPU & 8 GB\\ \hline
Vector width & 512 bits\\ \hline
\end{tabular}
\label{tab:mic}
\caption{Intel Xeon Phi}
\end{table}



%-----------------------------------------------------------------------------
% Xeon Phi
%-----------------------------------------------------------------------------

Apart from the characteristics showed in table \ref{tab:mic}, there are
others that should be mentioned. Each core contains a in-order dual
pipeline which can issue two instructions from the same hardware thread
per clock cycle. However, the front-end of the pipeline does not issue
instructions from the same hardware thread in consecutive cycles.\cite{Cepeda:PhiPerformance}

This means that the maximum issue rate is only attainable with at least
2 threads per core while the other threads have the purpose of hiding
pipeline stalls due to memory latency. %FIXME: Compor esta frase

The fact that the pipeline issue instructions in-order increases
memory related problems. %FIXME: Pode-se dar o exemplo do andrew

\begin{center}
\begin{figure}[htb]
    \includegraphics[width=\columnwidth]{img/phi_arch.jpg}
    \caption{Xeon Phi $\mu$Arch}
    \label{img:phi_arch}
\end{figure}
\end{center}


%-----------------------------------------------------------------------------
% Metis %FIXME: Pôr isto mais acima?
%-----------------------------------------------------------------------------

\section{Metis}
\label{sec:metis}

The version of Metis we used to perform measurement was 5.1.0

%-----------------------------------------------------------------------------
% Mt-metis
%-----------------------------------------------------------------------------

\section{Mt-metis}
\label{sec:mt-metis}

We used the 0.1 version of mtmetis.

\begin{center}
\begin{figure}[htb]
    \includegraphics[width=\columnwidth]{img/mtmetis128.eps}
    \caption{Mt-metis - 128 partitions}
    \label{img:mtmetis128}
\end{figure}
\end{center}


%-----------------------------------------------------------------------------
% GMetis and Galois Framework
%-----------------------------------------------------------------------------

\section{GMetis and the Galois Framework} %FIXME: Mudar o nome do titulo

\begin{center}
\begin{figure}[htb]
    \includegraphics[width=\columnwidth]{img/gmetis128.eps}
    \caption{GMetis - 128 partitions}
    \label{gmetis128}
\end{figure}
\end{center}

This figure shows the scalability of gmetis on Xeon phi over the runtime
with on thread. This example is for 128 partition, but we did
measurements for different number of partitions, such as 16 and 1024. All
of them have a similar behaviour.


%-----------------------------------------------------------------------------
% Enhancements
%-----------------------------------------------------------------------------

Throughout this month, we did some modifications to improve
\textit{GMetis} performance. We started by changing package mapping that
is done internally by the \textit{Galois} framework.

\textit{Galois} has support for \textit{NUMA} systems. This means that
it can run on different sockets (packets in Galois terminology) simultaneously.

Before an application starts running, \textit{Galois} parses the
cpuinfo file located in "/proc/cpuinfo" to create its packages.
\textit{Galois} normaly assigns a package to each socket. Therefore,
\textit{Galois} was assigning a package to the entire MIC coprocessor.

We changed that, but the results did not changed considerably.
Another characteristic was that galois was not prepared to deal with
processor that support more than hyperthread two thus when using diferent
thread values, some processor cores could have four threads running,
while others only one (default mapping). We also changed that so that the mapping could be
the most load balanced possible (load balanced mapping). For instance,
when running the application with 121 threads, with the default mapping,
the first 20 cores will run with four threads while the others will only
run with one thread. With the load balance mapping, only the last core
will run one thread, while the others will run 2 threads.

Although the Xeon Phi contains 61 cores, you should remember that the
operating system also needs to run. Therefore, only 60 cores may be
available for computations tasks.

%TODO: Talvez cagar leis sobre worklists bem como packages e para que
%servem, nomeadamente, loadbalancing
%TODO: Nao se deve correr em 61 cores. Devia-se se calhar mudar outra
%vez a parte de mapping para ver se faz alguma intreferencia

Unfortunately, Intel VTune only has support for Stampede's hosts, and
not for the coprocessor. Although, Intel states that profiling and
improving an application on the host gives similar improvements on MIC,
supports for profiling the MIC would be welcome, as there are important
differences in the architectures.

We profiled with the help of simple timers as well as with Papi, and we
found the most time consuming function, which is findMatching. This
function iterates through the graph's nodes, trying to match each node
with one available neighbor.

%TODO: Falar um pouco mais sobre isso, dizer que depois ficam matched, e
%que por isso, alguns nós podem ficar sozinhos (pk nao têm nodos livres
%para fazer match)

There are different ways to match the graph's nodes. The ones used are
"Heavy Weight Match" and "Random Match". The first iterates through each
node and match them with the neighbor whose shared edge has the most
weight. The second matches each node with the first neighbor node that
has not been matched yet.

Previously, these were used separately, i.e., only one of them was
used in the application. Using the two combined proved to be a better
solution, as the performance improved, and the edgecut remained the
same. Random Match is used in the first two iterations of the coarsening
phase. This improved runtime because, RM computes faster, as it does not
need to iterate through all neighbors when there is a node that has not
been matched. The algorithm is used only on the first two iterations
because the graph is larger on these iterations, and using RM instead of
HEM on small graphs does not proves to be faster and can actually worsen
edgecut.

%TODO: Falar do gcc e do icc
A deeper look into the assembly code generated by the two compilers
shows that gcc does not introduce prefetch instructions (even when using
\_\_builtin\_prefetch) as oposed to icc that prefetches. The results,
however do not differences on runtime.

We also did some tests with different worklist schedulers provided by
Galois. AltChunkedLIFO<8> was the fastest and it is actually the more
scalable one.


%-----------------------------------------------------------------------------
% Metis comparison
%-----------------------------------------------------------------------------


Some extra measurements allowed us to find that with 45-50 partitions
, GMetis start to run faster than mt-metis.

%-----------------------------------------------------------------------------
% Edgecut
%-----------------------------------------------------------------------------



%-----------------------------------------------------------------------------
% Conclusion
%-----------------------------------------------------------------------------

\section{Conclusion}
\label{sec:conc}
Results showed that both \textit{Metis} and \textit{Mt-metis} have
better edgecut than \textit{Gmetis}. However, Gmetis's runtime is lower
for a high number of partitions.

%FIXME: Adicionar o número de partições em que o GMetis passa a ser mais
%rápido

Xeon Phi provides a theoretical performance of 2112 GFlop/sec for double
precision arithmetic and 1056 GFlops/sec for single precision
arithmetic. This values comprises the use of 60 cores since one core is
necessary to perform operating system operations.

%TODO: Falar sobre o facto do Sampede não ter o VTune com suporte para
%os MICs?


%-----------------------------------------------------------------------------
% Bibliography
%-----------------------------------------------------------------------------

\bibliographystyle{plain}
\bibliography{references}


\end{document}
